# Reproducibility and Governance

---

> **Field** — Research Methodology, Data Governance
> **Scope** — Ensuring experiments are repeatable,
> auditable, and well-documented through seed control,
> versioning, logging, and change management practices

---

## Overview

Reproducibility means that anyone can re-run your
analysis and get the same results. Governance means
that every decision, change, and output is documented
and traceable. Together, they ensure your data science
work is trustworthy, auditable, and defensible. These
are not optional extras; they are requirements for
any serious research or production system.

---

## Definitions

### `Reproducibility`

**Definition.**
Reproducibility means that given the same data, code,
and configuration, the same results are produced every
time. No randomness, no hidden dependencies, and no
"it works on my machine" problems. Anyone should be
able to clone your repository and reproduce your exact
results.

**Context.**
Reproducibility is the foundation of scientific
credibility. In industry, it means you can audit past
decisions ("why did the model recommend this?"). In
research, it means others can verify and build upon
your work. Lack of reproducibility is one of the
biggest problems in modern data science.

**Example.**
A reproducibility checklist:

```
1. Fixed random seeds (Python, NumPy, etc.)
2. Pinned library versions (requirements.txt)
3. Data versioned or archived
4. Configuration stored as code/config files
5. Results generated by a single command
6. Environment documented (OS, Python version)
7. Git commit SHA recorded with each result
```

Minimal reproducible setup:

```python
# At top of every script
import random
import numpy as np

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# If using PyTorch
import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

# If using TensorFlow
import tensorflow as tf
tf.random.set_seed(SEED)
```

---

### `Auditability`

**Definition.**
Auditability means that every step of your analysis
can be traced back to its inputs, code, and decisions.
An auditor should be able to follow the chain from
raw data to final result and understand exactly what
happened at each step, who did it, and why.

**Context.**
Auditability is required by regulation in many
industries (finance, healthcare, government). Even
without regulation, auditability protects you: when
something goes wrong, you can trace back to find the
cause. It also builds trust with stakeholders who
need to understand how conclusions were reached.

**Example.**
```python
import json
import subprocess
from datetime import datetime

def create_audit_record(
    experiment_name, config, results
):
    """Create an audit trail entry."""
    record = {
        "timestamp": datetime.now().isoformat(),
        "experiment": experiment_name,
        "git_sha": subprocess.check_output(
            ["git", "rev-parse", "HEAD"]
        ).decode().strip(),
        "git_dirty": bool(
            subprocess.check_output(
                ["git", "status", "--porcelain"]
            ).decode().strip()
        ),
        "config": config,
        "results": results,
        "python_version": "3.11.5",
        "user": "simon"
    }

    # Append to audit log (JSONL format)
    with open("audit_log.jsonl", "a") as f:
        f.write(json.dumps(record) + "\n")

    return record
```

Key audit questions:
- What data was used? (version, source, hash)
- What code was run? (git SHA, script name)
- What config was used? (parameters, thresholds)
- What were the results? (metrics, outputs)
- When was it run? (timestamp)
- Who ran it? (user, role)

---

### `Governance`

**Definition.**
Data science governance is the set of rules, roles,
and processes that control how data is accessed,
models are developed, and decisions are made. It
defines who can do what, what approvals are needed,
and how changes are tracked and reviewed.

**Context.**
Governance prevents chaos in team environments. Without
it, people overwrite each other's work, deploy untested
models, or use data they should not have access to.
Good governance feels invisible when everything works
but is essential when things go wrong or when external
auditors ask questions.

**Example.**
A minimal governance framework:

```
Data Access:
- Raw data: read-only, access logged
- Processed data: team can read/write
- Production data: restricted, requires approval

Model Lifecycle:
- Development: any team member
- Testing: requires peer review
- Staging: requires lead approval
- Production: requires sign-off from lead
  and stakeholder

Change Management:
- All changes via pull requests
- Minimum one reviewer per PR
- Automated tests must pass
- Deployment log maintained
```

Governance roles:

```
Data Owner:
  Controls access to specific datasets

Model Owner:
  Responsible for model performance
  and monitoring

Reviewer:
  Approves code and model changes

Auditor:
  Verifies compliance with policies
```

---

### `Determinism`

**Definition.**
Determinism means that a process always produces the
same output given the same input. There is zero
randomness. Every function call, every computation,
and every file output is identical on every run.

**Context.**
Determinism is harder to achieve than it seems. Hidden
sources of randomness include: hash seed randomization
in Python, floating point operation ordering in
parallel code, random weight initialization in neural
networks, and system clock-based seeds. Achieving full
determinism requires controlling all of these.

**Example.**
Common sources of non-determinism and their fixes:

```python
import os
import random
import numpy as np

# 1. Python hash randomization
os.environ["PYTHONHASHSEED"] = "0"

# 2. Python random module
random.seed(42)

# 3. NumPy random
np.random.seed(42)

# 4. Dictionary ordering (Python 3.7+)
# Dicts are insertion-ordered by default
# but sets are not!
# Use sorted() when iterating sets:
my_set = {"c", "a", "b"}
for item in sorted(my_set):  # deterministic
    print(item)

# 5. Parallel processing order
# Always sort results after parallel execution
from concurrent.futures import ProcessPoolExecutor

with ProcessPoolExecutor() as ex:
    results = list(ex.map(func, data))
# results order matches input order (map)
# but if using submit(), sort explicitly
```

---

### `Seed Control`

**Definition.**
Seed control means explicitly setting the random
number generator seed at the start of every experiment.
A seed is a number that initializes the random number
generator to a specific state, ensuring the same
sequence of "random" numbers every time.

**Context.**
Seeds are the most basic tool for reproducibility.
Without them, every run of your experiment produces
slightly different results, making it impossible to
compare changes or debug issues. Always set seeds at
the very beginning of your script, before any library
imports that use randomness.

**Example.**
```python
import os
import random
import numpy as np

def set_all_seeds(seed=42):
    """Set seeds for all random generators."""
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)

    try:
        import torch
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    except ImportError:
        pass

    try:
        import tensorflow as tf
        tf.random.set_seed(seed)
    except ImportError:
        pass

# Call at the VERY START of every script
set_all_seeds(42)
```

Best practices:
- Store the seed in your config file
- Log the seed with every experiment result
- Use different seeds for repeated experiments
  to test stability (e.g., seeds 42, 43, 44)

---

### `Config Versioning`

**Definition.**
Config versioning means storing all experiment
parameters in a configuration file that is tracked
by version control. Instead of hard-coding values in
your script, you put them in a YAML, JSON, or TOML
file that is committed to Git alongside your code.

**Context.**
Config versioning answers the question "what
parameters did we use for that experiment last month?"
Without it, you forget settings, lose track of what
changed between runs, and cannot reproduce past
results. The config file is the single source of truth
for all tunable parameters.

**Example.**
A YAML config file (`config.yaml`):

```yaml
experiment:
  name: "anomaly_detection_v3"
  seed: 42
  description: "Tuned threshold experiment"

data:
  train_path: "data/train.parquet"
  test_path: "data/test.parquet"
  validation_split: 0.2

model:
  type: "isolation_forest"
  contamination: 0.05
  n_estimators: 200
  max_samples: 0.8

thresholds:
  alert: 0.85
  critical: 0.95

output:
  results_dir: "results/v3/"
  save_model: true
```

Loading config in Python:

```python
import yaml

with open("config.yaml") as f:
    config = yaml.safe_load(f)

seed = config["experiment"]["seed"]
model_type = config["model"]["type"]
threshold = config["thresholds"]["alert"]
```

Track with Git:

```bash
git add config.yaml
git commit -m "v3: tuned alert threshold to 0.85"
```

---

### `Artifact Naming`

**Definition.**
Artifact naming is a convention for naming output
files (models, reports, datasets) so that each artifact
can be traced back to the experiment that created it.
Good naming includes the experiment name, version,
date, and key parameters.

**Context.**
Without naming conventions, your results directory
quickly becomes a graveyard of unnamed files:
`model.pkl`, `model_v2.pkl`, `model_final.pkl`,
`model_FINAL_FINAL.pkl`. Systematic naming eliminates
confusion and enables automated tracking.

**Example.**
A naming convention:

```
{project}_{version}_{date}_{description}.{ext}

Examples:
  anomaly_v03_20260223_tuned_threshold.pkl
  forecast_v01_20260215_prophet_baseline.parquet
  report_v02_20260220_scaling_benchmark.md
```

Automated naming in Python:

```python
from datetime import datetime

def artifact_name(project, version,
                  description, ext):
    """Generate standardized artifact name."""
    date = datetime.now().strftime("%Y%m%d")
    safe_desc = description.replace(" ", "_")
    return (
        f"{project}_v{version:02d}"
        f"_{date}_{safe_desc}.{ext}"
    )

# Usage
model_name = artifact_name(
    "anomaly", 3, "tuned threshold", "pkl"
)
print(model_name)
# anomaly_v03_20260223_tuned_threshold.pkl

report_name = artifact_name(
    "scaling", 1, "dask benchmark", "json"
)
print(report_name)
# scaling_v01_20260223_dask_benchmark.json
```

---

### `JSONL Logging`

**Definition.**
JSONL (JSON Lines) is a file format where each line
is a separate, complete JSON object. It is used for
logging experiment metadata, metrics, and events.
Each line is self-contained, making it easy to append
new records without reading the entire file.

**Context.**
JSONL is ideal for experiment logging because you can
append results without loading previous results into
memory. Each experiment run adds one line. You can
later filter, sort, and analyze the log using standard
tools. It is human-readable but also machine-parseable.

**Example.**
Writing JSONL logs:

```python
import json
from datetime import datetime

def log_experiment(log_path, record):
    """Append one experiment record to log."""
    record["timestamp"] = (
        datetime.now().isoformat()
    )
    with open(log_path, "a") as f:
        f.write(json.dumps(record) + "\n")

# Log an experiment
log_experiment("experiments.jsonl", {
    "experiment": "anomaly_v3",
    "seed": 42,
    "f1_score": 0.87,
    "mae": 2.34,
    "config": "config_v3.yaml",
    "git_sha": "abc123f"
})
```

Reading JSONL logs:

```python
import json
import pandas as pd

# Read all experiments into a DataFrame
records = []
with open("experiments.jsonl") as f:
    for line in f:
        records.append(json.loads(line))

df = pd.DataFrame(records)
print(df[["experiment", "f1_score", "timestamp"]])

# Filter to best runs
best = df.nlargest(5, "f1_score")
print(best)
```

---

### `Commit SHA Recording`

**Definition.**
Commit SHA recording means saving the exact Git
commit hash with every experiment result. The SHA is
a unique identifier for the state of your code at the
time the experiment was run. It lets you check out
that exact code version later.

**Context.**
Code changes constantly. If you run an experiment
today and need to understand or reproduce it next
month, you need to know exactly which version of the
code was used. The commit SHA is the definitive answer.
Combined with a config file, it gives you complete
reproducibility.

**Example.**
Automatically capture the commit SHA:

```python
import subprocess

def get_git_info():
    """Get current git state."""
    sha = subprocess.check_output(
        ["git", "rev-parse", "HEAD"]
    ).decode().strip()

    branch = subprocess.check_output(
        ["git", "rev-parse",
         "--abbrev-ref", "HEAD"]
    ).decode().strip()

    # Check for uncommitted changes
    dirty = bool(
        subprocess.check_output(
            ["git", "status", "--porcelain"]
        ).decode().strip()
    )

    return {
        "commit_sha": sha,
        "branch": branch,
        "dirty": dirty
    }

# Usage
git_info = get_git_info()
print(f"SHA: {git_info['commit_sha']}")
print(f"Branch: {git_info['branch']}")
print(f"Dirty: {git_info['dirty']}")

# Include in experiment log
experiment_record = {
    "results": {"f1": 0.87},
    "git": git_info
}
```

If `dirty` is True, it means there were
uncommitted changes when the experiment ran.
This is a warning: the exact code state cannot
be recovered from the SHA alone.

Best practice: always commit before running
experiments so `dirty` is False.

---

### `Experiment Tracking`

**Definition.**
Experiment tracking is the systematic recording of
every experiment run, including its parameters,
metrics, code version, data version, and outputs.
It creates a searchable history of all experiments
so you can compare runs, find the best configuration,
and understand what has been tried.

**Context.**
Without tracking, teams repeat experiments, forget
what they tried, and lose promising results. Tracking
tools range from simple JSONL logs to platforms like
MLflow, Weights & Biases, and Neptune. The core idea
is the same: record everything, automatically, every
time you run an experiment.

**Example.**
Simple experiment tracker:

```python
import json
import os
from datetime import datetime

class ExperimentTracker:
    def __init__(self, log_dir="experiments"):
        os.makedirs(log_dir, exist_ok=True)
        self.log_path = os.path.join(
            log_dir, "tracker.jsonl"
        )

    def log_run(self, name, config, metrics):
        record = {
            "name": name,
            "timestamp": (
                datetime.now().isoformat()
            ),
            "config": config,
            "metrics": metrics,
            "git_sha": get_git_sha()
        }
        with open(self.log_path, "a") as f:
            f.write(json.dumps(record) + "\n")
        print(f"Logged: {name}")

    def get_best(self, metric="f1_score"):
        records = []
        with open(self.log_path) as f:
            for line in f:
                records.append(json.loads(line))
        best = max(
            records,
            key=lambda r: r["metrics"].get(
                metric, 0
            )
        )
        return best

# Usage
tracker = ExperimentTracker()

tracker.log_run(
    "anomaly_v3",
    config={"threshold": 0.85, "seed": 42},
    metrics={"f1_score": 0.87, "precision": 0.91}
)

best = tracker.get_best("f1_score")
print(f"Best run: {best['name']}")
```

Using MLflow (industry standard):

```bash
pip install mlflow
```

```python
import mlflow

mlflow.set_experiment("anomaly_detection")

with mlflow.start_run():
    mlflow.log_param("threshold", 0.85)
    mlflow.log_param("seed", 42)
    mlflow.log_metric("f1_score", 0.87)
    mlflow.log_artifact("model.pkl")
```

---

### `Change Documentation`

**Definition.**
Change documentation is the practice of recording
what was changed, why it was changed, and what the
impact was, every time you modify code, data, or
configuration. It creates a narrative history of the
project's evolution.

**Context.**
Git commit messages are the most basic form of change
documentation, but they are often not enough. A
CHANGELOG file, detailed PR descriptions, and inline
comments explain the reasoning behind changes. This
is invaluable when onboarding new team members or
when you need to understand a decision made months
ago.

**Example.**
A CHANGELOG entry:

```
## [v3.0] - 2026-02-23

### Changed
- Alert threshold raised from 0.75 to 0.85
  to reduce false positives (was generating
  50+ alerts/day; target is < 10).
- Switched from Isolation Forest to
  Local Outlier Factor for the anomaly
  detection model (5% F1 improvement).

### Added
- Energy cost simulation for MARL agents.
- Trust-weighted aggregation to replace
  simple averaging.

### Fixed
- Seed not being set before data splitting,
  causing non-reproducible train/test splits.
```

Good commit messages:

```bash
# Bad
git commit -m "updated model"

# Good
git commit -m "Raise alert threshold to 0.85

Reduces daily false positives from ~50 to ~8.
Based on analysis of 30-day alert log.
See experiments.jsonl run anomaly_v3_threshold."
```

---

### `Version Control`

**Definition.**
Version control is a system that tracks changes to
files over time, allowing you to recall any previous
version. Git is by far the most common version control
system. Every change is recorded as a commit with a
timestamp, author, and description.

**Context.**
Version control is non-negotiable for any data science
project. It protects against accidental deletions,
enables collaboration, provides a complete history
of changes, and integrates with code review and CI/CD
pipelines. If your work is not in Git, it effectively
does not exist in a professional context.

**Example.**
Essential Git workflow for data science:

```bash
# Initialize a repository
git init

# Track your code
git add src/ notebooks/ config.yaml
git commit -m "Initial project structure"

# Create a branch for experiments
git checkout -b experiment/new-threshold

# Make changes, test, commit
git add config.yaml src/model.py
git commit -m "Test threshold 0.85 for alerts"

# Push and create a pull request
git push -u origin experiment/new-threshold
gh pr create --title "Tune alert threshold"

# After review, merge to main
git checkout main
git merge experiment/new-threshold
```

What to track in Git:

```
Track:
  - Source code (.py, .R)
  - Configuration files (.yaml, .json)
  - Notebooks (.ipynb)
  - Requirements (requirements.txt)
  - Documentation (.md)
  - Small reference data

Do NOT track:
  - Large data files (use DVC or Git LFS)
  - Model binaries (use artifact storage)
  - Secrets (.env, API keys)
  - Generated outputs (results/, plots/)
```

`.gitignore` example:

```
data/raw/
data/processed/
*.pkl
*.parquet
.env
__pycache__/
.ipynb_checkpoints/
```

---

### `Peer Review`

**Definition.**
Peer review is the process of having another person
examine your code, analysis, or results before they
are accepted. The reviewer checks for correctness,
clarity, and adherence to standards. It is the most
effective quality control mechanism in both science
and software engineering.

**Context.**
Peer review catches mistakes that the author cannot
see. In data science, common issues found in review
include: data leakage between train and test sets,
incorrect metric calculations, missing edge cases,
and unclear documentation. Code review via pull
requests is the standard implementation.

**Example.**
A pull request review checklist for data science:

```
Code Quality:
- [ ] Code runs without errors
- [ ] Functions have docstrings
- [ ] Variable names are descriptive
- [ ] No hard-coded paths or magic numbers

Data Handling:
- [ ] No data leakage (train/test separation)
- [ ] Missing values handled explicitly
- [ ] Data types are correct
- [ ] Edge cases considered

Reproducibility:
- [ ] Random seeds set
- [ ] Config file updated
- [ ] Requirements file updated
- [ ] Git SHA will be recorded

Results:
- [ ] Metrics match expectations
- [ ] Comparison to baseline included
- [ ] Visualizations are labeled
- [ ] Conclusions follow from evidence
```

In GitHub:

```bash
# Create a review-ready PR
gh pr create \
    --title "Raise alert threshold to 0.85" \
    --body "Reduces false positives from 50
to 8 per day. F1 remains at 0.87.
See experiments.jsonl for details." \
    --reviewer colleague-username

# Review someone else's PR
gh pr review 42 --approve
gh pr review 42 --request-changes \
    --body "Need to add seed control"
```

---

## See Also

- [Scaling and Distributed Processing](./11_scaling_and_distributed_processing.md)
- [Anomaly Detection and Operational ML](./12_anomaly_detection_and_operational_ml.md)
- [Federated and Distributed Learning](./13_federated_and_distributed_learning.md)

---

> **Author** — Simon Parris | Data Science Reference Library
